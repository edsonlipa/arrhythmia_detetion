{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p5xWm9nrY-AU"
   },
   "source": [
    "#Clasificacion de arritmias cardicas en Electrocardigramas (ECG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6fZ_A6NLZM8u"
   },
   "source": [
    "* Instalando paquetes necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "wfgMyniTVK0B",
    "outputId": "2ff018d6-7e7d-42be-9de1-7478e89731d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wfdb\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/96/c2200539fdf4f087e14d30ed62a66544b6f441196bcb8ecc7a29ec6503b9/wfdb-2.2.1.tar.gz (94kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 8.5MB/s \n",
      "\u001b[?25hCollecting nose>=1.3.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 20.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from wfdb) (1.17.4)\n",
      "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.6/dist-packages (from wfdb) (3.1.2)\n",
      "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from wfdb) (2.21.0)\n",
      "Requirement already satisfied: pandas>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from wfdb) (0.25.3)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from wfdb) (1.3.3)\n",
      "Requirement already satisfied: sklearn>=0.0 in /usr/local/lib/python3.6/dist-packages (from wfdb) (0.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->wfdb) (2.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->wfdb) (2.6.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->wfdb) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->wfdb) (1.1.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->wfdb) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->wfdb) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->wfdb) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->wfdb) (2.8)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.1->wfdb) (2018.9)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn>=0.0->wfdb) (0.21.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib>=1.5.1->wfdb) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.5.1->wfdb) (42.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn>=0.0->wfdb) (0.14.0)\n",
      "Building wheels for collected packages: wfdb\n",
      "  Building wheel for wfdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wfdb: filename=wfdb-2.2.1-cp36-none-any.whl size=100368 sha256=313cb71811b9ae5fe12a679c4154ba8b12be6ed1f0b7197ef136b80cdeb6fb10\n",
      "  Stored in directory: /root/.cache/pip/wheels/bb/a9/00/0078d26b0c15b31be0001af8eb659496709c361c69641303f1\n",
      "Successfully built wfdb\n",
      "Installing collected packages: nose, wfdb\n",
      "Successfully installed nose-1.3.7 wfdb-2.2.1\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install wfdb\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "Tm8jjO_sUqRw",
    "outputId": "9a4fae21-3ffa-4a01-eed2-d51b7cdf1531"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "import wfdb\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pk\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score \n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IbmlpBIGaV1l"
   },
   "source": [
    "* ### importando google colab drive\n",
    "utlilizamos la libreria *drive* para poder utlizar los archivos en drive \n",
    "nos pide una autorizacion desde nuestra cuenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "3NHiPEkCaUyb",
    "outputId": "4f119045-3dcc-49b2-c16f-2cb2b8b13893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TaAE5Tj7armk",
    "outputId": "d8a2fdfb-89f2-4c75-d92d-8e07ea3c7101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/Colab Notebooks/ECG_Clasification\n"
     ]
    }
   ],
   "source": [
    "%cd gdrive/My\\ Drive/'Colab Notebooks'/'ECG_Clasification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XsT8FAR5Y0oi"
   },
   "outputs": [],
   "source": [
    "\n",
    "data_names = ['100', '101', '102', '103', '104', '105', '106', '107', \n",
    "              '108', '109', '111', '112', '113', '114', '115', '116', \n",
    "              '117', '118', '119', '121', '122', '123', '124', '200', \n",
    "              '201', '202', '203', '205', '207', '208', '209', '210', \n",
    "              '212', '213', '214', '215', '217', '219', '220', '221', \n",
    "              '222', '223', '228', '230', '231', '232', '233', '234']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GVYF0o-iaEct"
   },
   "outputs": [],
   "source": [
    "#tamnoho de las ventanas derecha e izquierdas\n",
    "widb = 99\n",
    "wida = 160\n",
    "n_samples = 650000 #<650000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "colab_type": "code",
    "id": "iyksiTn1a9Lp",
    "outputId": "672b2ffd-386d-4cd4-9693-24a9e93a1dbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient code  100\n",
      "patient code  101\n",
      "patient code  102\n",
      "patient code  103\n",
      "patient code  104\n",
      "patient code  105\n",
      "patient code  106\n",
      "patient code  107\n",
      "patient code  108\n",
      "patient code  109\n",
      "patient code  111\n",
      "patient code  112\n",
      "patient code  113\n",
      "patient code  114\n",
      "patient code  115\n",
      "patient code  116\n",
      "patient code  117\n",
      "patient code  118\n",
      "patient code  119\n",
      "patient code  121\n",
      "patient code  122\n",
      "patient code  123\n",
      "patient code  124\n",
      "patient code  200\n",
      "patient code  201\n",
      "patient code  202\n",
      "patient code  203\n",
      "patient code  205\n",
      "patient code  207\n",
      "patient code  208\n",
      "patient code  209\n",
      "patient code  210\n",
      "patient code  212\n",
      "patient code  213\n",
      "patient code  214\n",
      "patient code  215\n",
      "patient code  217\n",
      "patient code  219\n",
      "patient code  220\n",
      "patient code  221\n",
      "patient code  222\n",
      "patient code  223\n",
      "patient code  228\n",
      "patient code  230\n",
      "patient code  231\n",
      "patient code  232\n",
      "patient code  233\n",
      "patient code  234\n",
      "N : 75020\n",
      "L : 8072\n",
      "R : 7255\n",
      "A : 2546\n",
      "V : 7129\n",
      "Total NLRAV : 100022\n"
     ]
    }
   ],
   "source": [
    "cN=0\n",
    "cL=0\n",
    "cR=0\n",
    "cA=0\n",
    "cV=0\n",
    "labels = ['N', 'L', 'R', 'A', 'V']\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for d in data_names:\n",
    "    r=wfdb.rdrecord('./data/'+d,sampto=n_samples)\n",
    "    ann=wfdb.rdann('./data/'+d, 'atr', return_label_elements=['label_store', 'symbol'],sampto=n_samples)\n",
    "    if d!='114': # since 114 is reversed\n",
    "        sig = np.array(r.p_signal[:,0])\n",
    "    else:\n",
    "        sig = np.array(r.p_signal[:,1])\n",
    "    sig_len = len(sig)\n",
    "    #print(\"sig_len \",sig_len)\n",
    "    sym = ann.symbol\n",
    "    #print(\"ann.symbol \",sym)\n",
    "    #print(sig)\n",
    "    pos = ann.sample\n",
    "    #print(\"ann.sample \",pos)\n",
    "    beat_len = len(sym)\n",
    "    print(\"patient code \",d)\n",
    "    #print(\"beat_len \",beat_len)\n",
    "    \n",
    "    for i in range(beat_len):\n",
    "        if sym[i] in labels and pos[i]-widb>=0 and pos[i]+wida+1<=sig_len:\n",
    "        #if sym[i] in labels and pos[i]-wid>=0 and pos[i]+wid+1<=sig_len:\n",
    "            a = sig[pos[i]-widb:pos[i]+wida+1]\n",
    "            #print(a)\n",
    "            if len(a) != 260:\n",
    "                print(\"Length error\")\n",
    "                continue\n",
    "            X.append(a)\n",
    "            Y.append(labels.index(sym[i]))\n",
    "            #print('label',sym[i])\n",
    "            #print('sym[i] :',sym[i])\n",
    "            #print('labels.index(sym[i])   ',labels.index(sym[i]))\n",
    "            if(sym[i]=='N'):\n",
    "              cN=cN+1\n",
    "            elif(sym[i]=='L'):\n",
    "              cL=cL+1\n",
    "            elif(sym[i]=='R'):\n",
    "              cR=cR+1\n",
    "            elif(sym[i]=='A'):\n",
    "              cA=cA+1\n",
    "            elif(sym[i]=='V'):\n",
    "              cV=cV+1\n",
    "    #wfdb.plot_wfdb(record=r,annotation=ann,title=d+\" normal\",time_units='seconds')\n",
    "print('N :',cN)\n",
    "print('L :',cL)\n",
    "print('R :',cR)\n",
    "print('A :',cA)\n",
    "print('V :',cV)\n",
    "print('Total NLRAV :',len(X))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsvfq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    labels = ['N', 'S', 'V', 'F', 'Q']\n",
    "    sub_labels = ['N', 'L', 'R', 'e', 'j', 'A', 'a', 'J', 'S', 'V', 'E', 'F', '/', 'f', 'Q']\n",
    "    sub = {'N':'N', 'L':'N', 'R':'N', 'e':'N', 'j':'N', \n",
    "           'A':'S', 'a':'S', 'J':'S', 'S':'S',\n",
    "           'V':'V', 'E':'V',\n",
    "           'F':'F',\n",
    "           '/':'Q', 'f':'Q', 'Q':'Q'}\n",
    "    X = []\n",
    "    Y = []\n",
    "    for d in data_names:\n",
    "        r=wfdb.rdrecord('./data/'+d)\n",
    "        ann=wfdb.rdann('./data/'+d, 'atr', return_label_elements=['label_store', 'symbol'])\n",
    "        if d!='114':\n",
    "            sig = np.array(r.p_signal[:,0])\n",
    "        else:\n",
    "            sig = np.array(r.p_signal[:,1])\n",
    "        sig_len = len(sig)\n",
    "        sym = ann.symbol\n",
    "        pos = ann.sample\n",
    "        beat_len = len(sym)\n",
    "        for i in range(beat_len):\n",
    "            if sym[i] in labels and pos[i]-widb>=0 and pos[i]+wida+1<=sig_len:\n",
    "                a = sig[pos[i]-widb:pos[i]+wida+1]\n",
    "                if len(a) != 260:\n",
    "                    print(\"Length error\")\n",
    "                    continue\n",
    "                X.append(a)\n",
    "                Y.append(labels.index(sub[sym[i]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "fAjDeXEGZjR8",
    "outputId": "bf493842-a7eb-4f16-e813-b6ac52c88f27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82986, 260)\n",
      "(82986,)\n",
      "Counter({0: 75020, 2: 7129, 3: 802, 4: 33, 1: 2})\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "count = Counter(Y)\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UOzwLc6Ygb9J"
   },
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rfi9UZVGh0VG"
   },
   "outputs": [],
   "source": [
    "seed_ = 200 #se puede cambiar para variar los datos y reproducir los resultados\n",
    "data_len = len(X)\n",
    "np.random.seed(seed_)\n",
    "idx = list(range(data_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tl-Wgsuif1xm"
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(idx)\n",
    "\n",
    "train_len = int(data_len*0.6) # 60%\n",
    "valid_len = int(data_len*0.2) # 20%\n",
    "test_len = data_len-train_len-valid_len # 20%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "VMgjzCTSgbSL",
    "outputId": "7e90b301-e4c9-4290-a5b6-5a96aae0da2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49791, 260)\n",
      "(16597, 260)\n",
      "(16598, 260)\n",
      "Counter({0: 44982, 2: 4298, 3: 490, 4: 20, 1: 1})\n",
      "Counter({0: 15027, 2: 1395, 3: 167, 4: 8})\n",
      "Counter({0: 15011, 2: 1436, 3: 145, 4: 5, 1: 1})\n"
     ]
    }
   ],
   "source": [
    "X_train = X[idx][:train_len]\n",
    "X_valid = X[idx][train_len:train_len+valid_len]\n",
    "X_test = X[idx][train_len+valid_len:]\n",
    "Y_train = Y[idx][:train_len]\n",
    "Y_valid = Y[idx][train_len:train_len+valid_len]\n",
    "Y_test = Y[idx][train_len+valid_len:]\n",
    "\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(Counter(Y_train))\n",
    "print(Counter(Y_valid))\n",
    "print(Counter(Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5SG5qDqjhE2k"
   },
   "source": [
    "Expandiendo dimenciones para utilizar el parametro a modelo neuraonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "6iEfAjcVgl0T",
    "outputId": "96f6a45d-6ae4-48d4-f3f1-7fd8c23a30fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49791, 260, 1)\n",
      "(16597, 260, 1)\n",
      "(16598, 260, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_valid = np.expand_dims(X_valid, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "gFQsGnKMhKJe",
    "outputId": "104649b3-b30e-4d3e-ac17-e030edf6a6e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 ... 0 0 0]\n",
      "(49791, 5)\n"
     ]
    }
   ],
   "source": [
    "f_size = X_train.shape[1]\n",
    "class_num = 5\n",
    "\n",
    "lr = 0.01\n",
    "batch_size=32\n",
    "\n",
    "print(Y_train)\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes=class_num)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uKRzsH4mi3cK"
   },
   "outputs": [],
   "source": [
    "def make_model(model_type):\n",
    "    model = Sequential()\n",
    "    if model_type == '1D':\n",
    "        model.add(Conv1D(10, 3, activation='relu', input_shape=(f_size,1)))\n",
    "        model.add(MaxPooling1D(2))\n",
    "        model.add(Conv1D(10, 3, activation='relu'))\n",
    "        model.add(MaxPooling1D(2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(100, activation='relu'))\n",
    "    elif model_type == '1D-large':\n",
    "        model.add(Conv1D(50, 13, activation='relu', input_shape=(f_size,1)))\n",
    "        model.add(MaxPooling1D(2))\n",
    "        model.add(Conv1D(50, 13, activation='relu'))\n",
    "        model.add(MaxPooling1D(2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(100, activation='relu'))\n",
    "    elif model_type == 'LSTM':\n",
    "        model.add(LSTM(64, return_sequences=True, dropout=0.1, input_shape=(f_size, 1)))\n",
    "        model.add(LSTM(32, return_sequences=True, dropout=0.1))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "    elif model_type == 'BiLSTM':\n",
    "        model.add(Bidirectional(LSTM(64, return_sequences=True, dropout=0.1), merge_mode='sum', input_shape=(f_size, 1)))\n",
    "        model.add(Bidirectional(LSTM(32, return_sequences=True, dropout=0.1), merge_mode='sum'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "    elif model_type == 'ConvLSTM':\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(10, 7, activation='relu', input_shape=(260,1)))\n",
    "        model.add(MaxPooling1D(3))\n",
    "        model.add(Conv1D(10 ,7, activation='relu'))\n",
    "        model.add(MaxPooling1D(3))\n",
    "        model.add(LSTM(32, return_sequences=True,recurrent_dropout=0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "    model.add(Dense(class_num, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=lr), metrics=['accuracy'])\n",
    "    return model\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DWGKYDTIQj7I"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iDJ3hRhWK0HG"
   },
   "outputs": [],
   "source": [
    "def make_model_cnn_lstm():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(10, 7, activation='relu', input_shape=(260,1)))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Conv1D(10 ,7, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    #model.add(Conv1D(10 ,6, activation='relu'))\n",
    "    #model.add(MaxPooling1D(2))\n",
    "    #model.add(LSTM(64, return_sequences=True,recurrent_dropout=0.25))\n",
    "    model.add(LSTM(32, return_sequences=True,recurrent_dropout=0.25))\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=lr))\n",
    "    return model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-0Xusn6R283P"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "SJo1e72Fi4_h",
    "outputId": "2b66c3e9-9b62-4898-fb07-d6776495c1e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 258, 10)           40        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 129, 10)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 127, 10)           310       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 63, 10)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 630)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               63100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 63,955\n",
      "Trainable params: 63,955\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = make_model('1D')\n",
    "#model = make_model_cnn_lstm()\n",
    "best_SE = 0\n",
    "best_ACC = 0\n",
    "patience = 30\n",
    "pcnt = 0\n",
    "\n",
    "best_model = make_model('1D')\n",
    "\n",
    "bin_label = lambda x: min(1,x)\n",
    "#model.build()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TjBT2SvLncwW",
    "outputId": "5b6a7d0c-505e-478e-8c5c-15a0f97f5291"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60013/60013 [==============================] - 8s 138us/step - loss: 0.0492 - acc: 0.9875\n",
      "Epoch 2/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0463 - acc: 0.9882\n",
      "Epoch 3/100\n",
      "60013/60013 [==============================] - 9s 143us/step - loss: 0.0443 - acc: 0.9887\n",
      "Epoch 4/100\n",
      "60013/60013 [==============================] - 8s 140us/step - loss: 0.0398 - acc: 0.9894\n",
      "Epoch 5/100\n",
      "60013/60013 [==============================] - 8s 139us/step - loss: 0.0431 - acc: 0.9895\n",
      "Epoch 6/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0407 - acc: 0.9896\n",
      "Epoch 7/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0365 - acc: 0.9909\n",
      "Epoch 8/100\n",
      "60013/60013 [==============================] - 8s 138us/step - loss: 0.0402 - acc: 0.9905\n",
      "Epoch 9/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0426 - acc: 0.9903\n",
      "Epoch 10/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0334 - acc: 0.9916\n",
      "Epoch 11/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0357 - acc: 0.9912\n",
      "Epoch 12/100\n",
      "60013/60013 [==============================] - 8s 138us/step - loss: 0.0419 - acc: 0.9908\n",
      "Epoch 13/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0385 - acc: 0.9912\n",
      "Epoch 14/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0352 - acc: 0.9913\n",
      "Epoch 15/100\n",
      "60013/60013 [==============================] - 8s 139us/step - loss: 0.0337 - acc: 0.9920\n",
      "Epoch 16/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0342 - acc: 0.9920\n",
      "Epoch 17/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0342 - acc: 0.9919\n",
      "Epoch 18/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0369 - acc: 0.9915\n",
      "Epoch 19/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0404 - acc: 0.9910\n",
      "Epoch 20/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0307 - acc: 0.9929\n",
      "Epoch 21/100\n",
      "60013/60013 [==============================] - 8s 138us/step - loss: 0.0360 - acc: 0.9919\n",
      "Epoch 22/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0386 - acc: 0.9913\n",
      "Epoch 23/100\n",
      "60013/60013 [==============================] - 8s 138us/step - loss: 0.0341 - acc: 0.9924\n",
      "Epoch 24/100\n",
      "60013/60013 [==============================] - 8s 138us/step - loss: 0.0303 - acc: 0.9934\n",
      "Epoch 25/100\n",
      "60013/60013 [==============================] - 8s 139us/step - loss: 0.0394 - acc: 0.9918\n",
      "Epoch 26/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0338 - acc: 0.9930\n",
      "Epoch 27/100\n",
      "60013/60013 [==============================] - 8s 139us/step - loss: 0.0318 - acc: 0.9931\n",
      "Epoch 28/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0383 - acc: 0.9919\n",
      "Epoch 29/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0319 - acc: 0.9932\n",
      "Epoch 30/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0431 - acc: 0.9912\n",
      "Epoch 31/100\n",
      "60013/60013 [==============================] - 8s 135us/step - loss: 0.0291 - acc: 0.9941\n",
      "Epoch 32/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0314 - acc: 0.9932\n",
      "Epoch 33/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0347 - acc: 0.9925\n",
      "Epoch 34/100\n",
      "60013/60013 [==============================] - 8s 138us/step - loss: 0.0375 - acc: 0.9927\n",
      "Epoch 35/100\n",
      "60013/60013 [==============================] - 8s 138us/step - loss: 0.0343 - acc: 0.9927\n",
      "Epoch 36/100\n",
      "60013/60013 [==============================] - 8s 135us/step - loss: 0.0364 - acc: 0.9930\n",
      "Epoch 37/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0308 - acc: 0.9938\n",
      "Epoch 38/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0343 - acc: 0.9930\n",
      "Epoch 39/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0330 - acc: 0.9931\n",
      "Epoch 40/100\n",
      "60013/60013 [==============================] - 8s 138us/step - loss: 0.0390 - acc: 0.9925\n",
      "Epoch 41/100\n",
      "60013/60013 [==============================] - 9s 142us/step - loss: 0.0341 - acc: 0.9931\n",
      "Epoch 42/100\n",
      "60013/60013 [==============================] - 8s 138us/step - loss: 0.0380 - acc: 0.9932\n",
      "Epoch 43/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0333 - acc: 0.9937\n",
      "Epoch 44/100\n",
      "60013/60013 [==============================] - 8s 138us/step - loss: 0.0281 - acc: 0.9941\n",
      "Epoch 45/100\n",
      "60013/60013 [==============================] - 8s 138us/step - loss: 0.0329 - acc: 0.9933\n",
      "Epoch 46/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0370 - acc: 0.9930\n",
      "Epoch 47/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0434 - acc: 0.9926\n",
      "Epoch 48/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0336 - acc: 0.9939\n",
      "Epoch 49/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0381 - acc: 0.9937\n",
      "Epoch 50/100\n",
      "60013/60013 [==============================] - 8s 139us/step - loss: 0.0428 - acc: 0.9926\n",
      "Epoch 51/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0282 - acc: 0.9942\n",
      "Epoch 52/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0325 - acc: 0.9934\n",
      "Epoch 53/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0393 - acc: 0.9932\n",
      "Epoch 54/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0321 - acc: 0.9940\n",
      "Epoch 55/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0320 - acc: 0.9945\n",
      "Epoch 56/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0331 - acc: 0.9940\n",
      "Epoch 57/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0401 - acc: 0.9929\n",
      "Epoch 58/100\n",
      "60013/60013 [==============================] - 8s 139us/step - loss: 0.0422 - acc: 0.9930\n",
      "Epoch 59/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0377 - acc: 0.9937\n",
      "Epoch 60/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0317 - acc: 0.9943\n",
      "Epoch 61/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0416 - acc: 0.9931\n",
      "Epoch 62/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0302 - acc: 0.9944\n",
      "Epoch 63/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0478 - acc: 0.9927\n",
      "Epoch 64/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0454 - acc: 0.9934\n",
      "Epoch 65/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0423 - acc: 0.9935\n",
      "Epoch 66/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0393 - acc: 0.9939\n",
      "Epoch 67/100\n",
      "60013/60013 [==============================] - 8s 135us/step - loss: 0.0495 - acc: 0.9921\n",
      "Epoch 68/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0293 - acc: 0.9949\n",
      "Epoch 69/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0360 - acc: 0.9942\n",
      "Epoch 70/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0362 - acc: 0.9942\n",
      "Epoch 71/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0347 - acc: 0.9944\n",
      "Epoch 72/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0420 - acc: 0.9934\n",
      "Epoch 73/100\n",
      "60013/60013 [==============================] - 8s 138us/step - loss: 0.0328 - acc: 0.9948\n",
      "Epoch 74/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0361 - acc: 0.9943\n",
      "Epoch 75/100\n",
      "60013/60013 [==============================] - 8s 135us/step - loss: 0.0401 - acc: 0.9939\n",
      "Epoch 76/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0367 - acc: 0.9939\n",
      "Epoch 77/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0465 - acc: 0.9935\n",
      "Epoch 78/100\n",
      "60013/60013 [==============================] - 8s 139us/step - loss: 0.0466 - acc: 0.9936\n",
      "Epoch 79/100\n",
      "60013/60013 [==============================] - 8s 139us/step - loss: 0.0450 - acc: 0.9935\n",
      "Epoch 80/100\n",
      "60013/60013 [==============================] - 8s 138us/step - loss: 0.0433 - acc: 0.9936\n",
      "Epoch 81/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0590 - acc: 0.9921\n",
      "Epoch 82/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0546 - acc: 0.9925\n",
      "Epoch 83/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0409 - acc: 0.9938\n",
      "Epoch 84/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0329 - acc: 0.9947\n",
      "Epoch 85/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0480 - acc: 0.9937\n",
      "Epoch 86/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0436 - acc: 0.9939\n",
      "Epoch 87/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0396 - acc: 0.9939\n",
      "Epoch 88/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0399 - acc: 0.9942\n",
      "Epoch 89/100\n",
      "60013/60013 [==============================] - 8s 135us/step - loss: 0.0374 - acc: 0.9939\n",
      "Epoch 90/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0690 - acc: 0.9919\n",
      "Epoch 91/100\n",
      "60013/60013 [==============================] - 8s 135us/step - loss: 0.0394 - acc: 0.9948\n",
      "Epoch 92/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0465 - acc: 0.9942\n",
      "Epoch 93/100\n",
      "60013/60013 [==============================] - 8s 137us/step - loss: 0.0494 - acc: 0.9938\n",
      "Epoch 94/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0590 - acc: 0.9924\n",
      "Epoch 95/100\n",
      "60013/60013 [==============================] - 8s 140us/step - loss: 0.0423 - acc: 0.9942\n",
      "Epoch 96/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0450 - acc: 0.9938\n",
      "Epoch 97/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0461 - acc: 0.9940\n",
      "Epoch 98/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0506 - acc: 0.9935\n",
      "Epoch 99/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0437 - acc: 0.9941\n",
      "Epoch 100/100\n",
      "60013/60013 [==============================] - 8s 136us/step - loss: 0.0667 - acc: 0.9924\n",
      "shape predicted (20005, 5)\n",
      "predicted argmax axis = 1 [0 0 0 ... 0 1 0]\n",
      "predicted argmax shape (20005,)\n",
      "n samples correct predicted :  19782\n",
      "Epoch: 10 | SE: 0.9776 | ACC: 0.9889 | AUC: 0.9866 | SP: 0.9956\n",
      "timpo utlizado para entrenar  822.4323003292084\n"
     ]
    }
   ],
   "source": [
    "timecallback = TimeHistory()\n",
    "model.fit(X_train, Y_train, batch_size=batch_size,callbacks=[timecallback], epochs=100, verbose=1)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print('shape predicted',y_pred.shape)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print('predicted argmax axis = 1',y_pred)\n",
    "print('predicted argmax shape',y_pred.shape)\n",
    "correct_predicted = np.sum(y_pred==Y_test)\n",
    "print('n samples correct predicted : ',correct_predicted)\n",
    "acc = correct_predicted/len(Y_test)\n",
    "\n",
    "\n",
    "y_true = list(map(bin_label, Y_test))\n",
    "y_pred = list(map(bin_label, y_pred))\n",
    "auc = roc_auc_score(y_true, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "SE = tp/(tp+fn)\n",
    "SP = tn/(fp+tn)\n",
    "print(\"Epoch: %d | SE: %.4f | ACC: %.4f | AUC: %.4f | SP: %.4f\" %(10, SE, acc, auc, SP))\n",
    "print(\"timpo utlizado para entrenar \",sum(timecallback.times))\n",
    "    \n",
    "model.save(\"clasifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entrenamiento y validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | SE: 0.9083 | Best SE: 0.9720 | ACC: 0.9887 | Best ACC: 0.9919 | AUC: 0.9533 | SP: 0.9984\n",
      "Epoch: 2 | SE: 0.9631 | Best SE: 0.9720 | ACC: 0.9922 | Best ACC: 0.9919 | AUC: 0.9801 | SP: 0.9971\n",
      "Epoch: 3 | SE: 0.9713 | Best SE: 0.9720 | ACC: 0.9921 | Best ACC: 0.9919 | AUC: 0.9837 | SP: 0.9961\n",
      "Epoch: 4 | SE: 0.9592 | Best SE: 0.9720 | ACC: 0.9908 | Best ACC: 0.9919 | AUC: 0.9778 | SP: 0.9963\n",
      "Epoch: 5 | SE: 0.9567 | Best SE: 0.9720 | ACC: 0.9924 | Best ACC: 0.9919 | AUC: 0.9770 | SP: 0.9973\n",
      "Epoch: 6 | SE: 0.9459 | Best SE: 0.9720 | ACC: 0.9908 | Best ACC: 0.9919 | AUC: 0.9715 | SP: 0.9971\n",
      "Epoch: 7 | SE: 0.9592 | Best SE: 0.9720 | ACC: 0.9921 | Best ACC: 0.9919 | AUC: 0.9783 | SP: 0.9973\n",
      "Epoch: 8 | SE: 0.9611 | Best SE: 0.9720 | ACC: 0.9928 | Best ACC: 0.9919 | AUC: 0.9797 | SP: 0.9983\n",
      "Epoch: 9 | SE: 0.9529 | Best SE: 0.9720 | ACC: 0.9913 | Best ACC: 0.9919 | AUC: 0.9749 | SP: 0.9970\n",
      "Epoch: 10 | SE: 0.9586 | Best SE: 0.9720 | ACC: 0.9916 | Best ACC: 0.9919 | AUC: 0.9778 | SP: 0.9969\n",
      "Epoch: 11 | SE: 0.9503 | Best SE: 0.9720 | ACC: 0.9922 | Best ACC: 0.9919 | AUC: 0.9744 | SP: 0.9985\n",
      "Epoch: 12 | SE: 0.9484 | Best SE: 0.9720 | ACC: 0.9916 | Best ACC: 0.9919 | AUC: 0.9731 | SP: 0.9979\n",
      "Epoch: 13 | SE: 0.9382 | Best SE: 0.9720 | ACC: 0.9876 | Best ACC: 0.9919 | AUC: 0.9662 | SP: 0.9941\n",
      "Epoch: 14 | SE: 0.9510 | Best SE: 0.9720 | ACC: 0.9911 | Best ACC: 0.9919 | AUC: 0.9741 | SP: 0.9973\n",
      "Epoch: 15 | SE: 0.9414 | Best SE: 0.9720 | ACC: 0.9911 | Best ACC: 0.9919 | AUC: 0.9695 | SP: 0.9975\n",
      "Epoch: 16 | SE: 0.9344 | Best SE: 0.9720 | ACC: 0.9908 | Best ACC: 0.9919 | AUC: 0.9662 | SP: 0.9981\n",
      "Epoch: 17 | SE: 0.9554 | Best SE: 0.9720 | ACC: 0.9914 | Best ACC: 0.9919 | AUC: 0.9762 | SP: 0.9971\n",
      "Epoch: 18 | SE: 0.9567 | Best SE: 0.9720 | ACC: 0.9917 | Best ACC: 0.9919 | AUC: 0.9770 | SP: 0.9974\n",
      "Epoch: 19 | SE: 0.9510 | Best SE: 0.9720 | ACC: 0.9924 | Best ACC: 0.9919 | AUC: 0.9746 | SP: 0.9982\n",
      "Epoch: 20 | SE: 0.9510 | Best SE: 0.9720 | ACC: 0.9920 | Best ACC: 0.9919 | AUC: 0.9746 | SP: 0.9983\n",
      "Epoch: 21 | SE: 0.9484 | Best SE: 0.9720 | ACC: 0.9881 | Best ACC: 0.9919 | AUC: 0.9713 | SP: 0.9942\n",
      "Epoch: 22 | SE: 0.9586 | Best SE: 0.9720 | ACC: 0.9914 | Best ACC: 0.9919 | AUC: 0.9777 | SP: 0.9969\n",
      "Epoch: 23 | SE: 0.9529 | Best SE: 0.9720 | ACC: 0.9928 | Best ACC: 0.9919 | AUC: 0.9759 | SP: 0.9989\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save = 'best_train'\n",
    "for e in range(1, 100+1):\n",
    "\n",
    "    model.fit(X_train, Y_train, batch_size=batch_size, epochs=1, verbose=0)\n",
    "\n",
    "    y_pred = model.predict(X_valid)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    acc = np.sum(y_pred==Y_valid)/len(Y_valid)\n",
    "\n",
    "    y_true = list(map(bin_label, Y_valid))\n",
    "    y_pred = list(map(bin_label, y_pred))\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    SE = tp/(tp+fn)\n",
    "    SP = tn/(fp+tn)\n",
    "\n",
    "    if SE+acc > best_SE+best_ACC:\n",
    "        best_SE, best_ACC = SE, acc\n",
    "        best_model.set_weights(model.get_weights())\n",
    "        pcnt = 0\n",
    "    else:\n",
    "        pcnt += 1\n",
    "    \n",
    "    print(\"Epoch: %d | SE: %.4f | Best SE: %.4f | ACC: %.4f | Best ACC: %.4f | AUC: %.4f | SP: %.4f\" %(e, SE, best_SE, acc, best_ACC, auc, SP))\n",
    "    if pcnt==patience:\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        acc = np.sum(y_pred==Y_test)/len(Y_test)\n",
    "        y_true = list(map(bin_label, Y_test))\n",
    "        y_pred = list(map(bin_label, y_pred))\n",
    "        auc = roc_auc_score(y_true, y_pred)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        SE = tp/(tp+fn)\n",
    "        SP = tn/(fp+tn)\n",
    "        print(\"CNN Test | SE: %.4f | ACC: %.4f | AUC: %.4f | SP: %.4f | valid SE: %.4f | valid ACC: %.4f\" %(SE, acc, auc, SP, best_SE, best_ACC))\n",
    "        with open(\"./result/\"+save, \"a\") as fw:\n",
    "            fw.write(\"SE: %.4f | ACC: %.4f | AUC: %.4f | SP: %.4f | valid SE: %.4f | valid ACC: %.4f\\n\" %(SE, acc, auc, SP, best_SE, best_ACC))\n",
    "        break\n",
    "\n",
    "model.save('cnn_trained')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Vsl5SyzGFco"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import os.path as path\n",
    "\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "MODEL_NAME = 'clasificador'\n",
    "if not path.exists('out'):\n",
    "        os.mkdir('out')\n",
    "def export_model(saver, model, input_node_names, output_node_name):\n",
    "    tf.train.write_graph(K.get_session().graph_def, 'out', \\\n",
    "        MODEL_NAME + '_graph.pbtxt')\n",
    "\n",
    "    saver.save(K.get_session(), 'out/' + MODEL_NAME + '.chkp')\n",
    "\n",
    "    freeze_graph.freeze_graph('out/' + MODEL_NAME + '_graph.pbtxt', None, \\\n",
    "        False, 'out/' + MODEL_NAME + '.chkp', output_node_name, \\\n",
    "        \"save/restore_all\", \"save/Const:0\", \\\n",
    "        'out/frozen_' + MODEL_NAME + '.pb', True, \"\")\n",
    "\n",
    "    input_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.Open('out/frozen_' + MODEL_NAME + '.pb', \"rb\") as f:\n",
    "        input_graph_def.ParseFromString(f.read())\n",
    "\n",
    "    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "            input_graph_def, input_node_names, [output_node_name],\n",
    "            tf.float32.as_datatype_enum)\n",
    "\n",
    "    with tf.gfile.FastGFile('out/opt_' + MODEL_NAME + '.pb', \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "    print(\"graph saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "S6H_liZxI1U9",
    "outputId": "1586e15b-050d-4b3b-b6ba-d33e6fb33cbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from out/clasificador.chkp\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b2eda4b087e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mexport_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"conv2d_1_input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dense_2/Softmax\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-9c25e55c05d0>\u001b[0m in \u001b[0;36mexport_model\u001b[0;34m(saver, model, input_node_names, output_node_name)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'out/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.chkp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mfreeze_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'out/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_graph.pbtxt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'out/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.chkp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_node_name\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0;34m\"save/restore_all\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"save/Const:0\"\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0;34m'out/frozen_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0minput_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py\u001b[0m in \u001b[0;36mfreeze_graph\u001b[0;34m(input_graph, input_saver, input_binary, input_checkpoint, output_node_names, restore_op_name, filename_tensor_name, output_graph, clear_devices, initializer_nodes, variable_names_whitelist, variable_names_blacklist, input_meta_graph, input_saved_model_dir, saved_model_tags, checkpoint_version)\u001b[0m\n\u001b[1;32m    359\u001b[0m       \u001b[0minput_saved_model_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m       \u001b[0msaved_model_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m       checkpoint_version=checkpoint_version)\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py\u001b[0m in \u001b[0;36mfreeze_graph_with_def_protos\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    231\u001b[0m           \u001b[0moutput_node_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m           \u001b[0mvariable_names_whitelist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariable_names_whitelist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m           variable_names_blacklist=variable_names_blacklist)\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0;31m# Write GraphDef to file if output path has been given.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py\u001b[0m in \u001b[0;36mconvert_variables_to_constants\u001b[0;34m(sess, input_graph_def, output_node_names, variable_names_whitelist, variable_names_blacklist)\u001b[0m\n\u001b[1;32m    275\u001b[0m   \u001b[0;31m# This graph only includes the nodes needed to evaluate the output nodes, and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m   \u001b[0;31m# removes unneeded nodes like those involved in saving and assignment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m   \u001b[0minference_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_sub_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_node_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m   \u001b[0;31m# Identify the ops in the graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py\u001b[0m in \u001b[0;36mextract_sub_graph\u001b[0;34m(graph_def, dest_nodes)\u001b[0m\n\u001b[1;32m    195\u001b[0m   name_to_input_name, name_to_node, name_to_seq_num = _extract_graph_summary(\n\u001b[1;32m    196\u001b[0m       graph_def)\n\u001b[0;32m--> 197\u001b[0;31m   \u001b[0m_assert_nodes_are_present\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_to_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m   \u001b[0mnodes_to_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bfs_for_reachable_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_to_input_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py\u001b[0m in \u001b[0;36m_assert_nodes_are_present\u001b[0;34m(name_to_node, nodes)\u001b[0m\n\u001b[1;32m    150\u001b[0m   \u001b[0;34m\"\"\"Assert that nodes are present in the graph.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname_to_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%s is not in graph\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: dense_2/Softmax is not in graph"
     ]
    }
   ],
   "source": [
    "  input_node_name = 'input'\n",
    "  keep_prob_node_name = 'keep_prob'\n",
    "  output_node_name = 'output'\n",
    "\n",
    "\n",
    "  \n",
    "  export_model(tf.train.Saver(), model, [\"conv2d_1_input\"], \"dense_2/Softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "VJLh9T7c-sYZ",
    "outputId": "10e84cd5-0f79-4628-d7a4-232515aca45e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14931,    71,   150,  4853])"
      ]
     },
     "execution_count": 90,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true, y_pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QBpRwQAW-nap",
    "outputId": "fe8ca77c-b390-42ca-a66b-6ea92df3f8bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: 1D\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "60013/60013 [==============================] - 15s 252us/step - loss: 0.1665 - acc: 0.9563\n",
      "Epoch 2/2\n",
      "60013/60013 [==============================] - 8s 134us/step - loss: 0.0942 - acc: 0.9761\n",
      "Epoch: 2 | ACC: 0.9749 | SE: 0.9362 | SP: 0.9980 | AUC: 0.9671 \n",
      "tiempo utlizado para entrenar: 23.18 segundos\n",
      "Modelo: 1D-large\n",
      "Epoch 1/2\n",
      "60013/60013 [==============================] - 9s 148us/step - loss: 0.1592 - acc: 0.9590\n",
      "Epoch 2/2\n",
      "60013/60013 [==============================] - 9s 143us/step - loss: 0.1051 - acc: 0.9745\n",
      "Epoch: 2 | ACC: 0.9798 | SE: 0.9654 | SP: 0.9931 | AUC: 0.9793 \n",
      "tiempo utlizado para entrenar: 17.43 segundos\n",
      "Modelo: LSTM\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Epoch 1/2\n",
      "60013/60013 [==============================] - 1477s 25ms/step - loss: 0.3348 - acc: 0.9067\n",
      "Epoch 2/2\n",
      "60013/60013 [==============================] - 1472s 25ms/step - loss: 0.1577 - acc: 0.9581\n",
      "Epoch: 2 | ACC: 0.9768 | SE: 0.9524 | SP: 0.9955 | AUC: 0.9740 \n",
      "tiempo utlizado para entrenar: 2949.17 segundos\n",
      "Modelo: BiLSTM\n",
      "Epoch 1/2\n",
      "60013/60013 [==============================] - 2958s 49ms/step - loss: 4.0681 - acc: 0.7468\n",
      "Epoch 2/2\n",
      "60013/60013 [==============================] - 2967s 49ms/step - loss: 4.0703 - acc: 0.7475\n",
      "Epoch: 2 | ACC: 0.7499 | SE: 0.0000 | SP: 1.0000 | AUC: 0.5000 \n",
      "tiempo utlizado para entrenar: 5924.72 segundos\n",
      "Modelo: ConvLSTM\n",
      "Epoch 1/2\n",
      "60013/60013 [==============================] - 89s 1ms/step - loss: 0.2267 - acc: 0.9352\n",
      "Epoch 2/2\n",
      "60013/60013 [==============================] - 87s 1ms/step - loss: 0.1098 - acc: 0.9696\n",
      "Epoch: 2 | ACC: 0.9807 | SE: 0.9574 | SP: 0.9956 | AUC: 0.9765 \n",
      "tiempo utlizado para entrenar: 175.93 segundos\n"
     ]
    }
   ],
   "source": [
    "bin_label = lambda x: min(1,x)\n",
    "models = ['1D','1D-large','LSTM','BiLSTM','ConvLSTM']\n",
    "\n",
    "for mod in models:\n",
    "  print(\"Modelo:\",mod)\n",
    "  model = make_model(mod) \n",
    "  timecallback = TimeHistory()\n",
    "  model.fit(X_train, Y_train, batch_size=batch_size,callbacks=[timecallback], epochs=2, verbose=1)\n",
    "\n",
    "  y_pred = model.predict(X_test)\n",
    "  y_pred = np.argmax(y_pred, axis=1)\n",
    "  correct_predicted = np.sum(y_pred==Y_test)\n",
    "  #print('%d ejemplos correctamente clasificados de %d : ',correct_predicted,len(Y_test))\n",
    "  acc = correct_predicted/len(Y_test)\n",
    "\n",
    "  y_true = list(map(bin_label, Y_test))\n",
    "  y_pred = list(map(bin_label, y_pred))\n",
    "  auc = roc_auc_score(y_true, y_pred)\n",
    "  tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "  SE = tp/(tp+fn)\n",
    "  SP = tn/(fp+tn)\n",
    "  print(\"Epoch: %d | ACC: %.4f | SE: %.4f | SP: %.4f | AUC: %.4f \" %(2,acc, SE, SP, auc))\n",
    "  print(\"tiempo utlizado para entrenar: %.2f segundos\"%sum(timecallback.times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAfcegCD6xap"
   },
   "outputs": [],
   "source": [
    "#make convolutional autoencoder\n",
    "# ENCODER\n",
    "input_sig = Input(batch_shape=(None,260,1))\n",
    "x = Conv1D(16,5, activation='relu', padding='valid')(input_sig)\n",
    "x1 = MaxPooling1D(2)(x)\n",
    "x2 = Conv1D(64,5, activation='relu', padding='valid')(x1)\n",
    "x3 = BatchNormalization()(x2)\n",
    "x4 = MaxPooling1D(2)(x3)\n",
    "x5 = Conv1D(32,3, activation='relu', padding='valid')(x4)\n",
    "x6 = Conv1D(1,3, activation='relu', padding='valid')(x5)\n",
    "encoded = MaxPooling1D(2)(x6)\n",
    "#encoded = Dense(32,activation = 'relu')(flat)\n",
    " \n",
    "print(\"shape of encoded {}\".format(K.int_shape(encoded)))\n",
    " \n",
    "# DECODER \n",
    "x6_ = Conv1D(1,3, activation='relu', padding='valid')(x7)\n",
    "x5_ = Conv1D(32,3, activation='relu', padding='valid')(x6_)\n",
    "x4_ = MaxPooling1D(2)(x5_)\n",
    "x3_ = BatchNormalization()(x4_)\n",
    "x2_ = Conv1D(64,5, activation='relu', padding='valid')(x3_)\n",
    "x1_ = MaxPooling1D(2)(x_2)\n",
    "x_ = Conv1D(16,5, activation='relu', padding='valid')(x_1)\n",
    "upsamp = UpSampling1D(2)(x_)\n",
    "flat = Flatten()(upsamp)\n",
    "decoded = Dense(260,activation = 'sigmoid')(flat)\n",
    " \n",
    "print(\"shape of decoded {}\".format(K.int_shape(decoded)))\n",
    " \n",
    "autoencoder = Model(input_sig, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gSwaXeujnU1Z"
   },
   "outputs": [],
   "source": [
    "for e in range(1, 15049+1):\n",
    "\n",
    "    model.fit(X_train, Y_train, batch_size=batch_size, epochs=1, verbose=0)\n",
    "\n",
    "    y_pred = model.predict(X_valid)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    acc = np.sum(y_pred==Y_valid)/len(Y_valid)\n",
    "\n",
    "    y_true = list(map(bin_label, Y_valid))\n",
    "    y_pred = list(map(bin_label, y_pred))\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    SE = tp/(tp+fn)\n",
    "    SP = tn/(fp+tn)\n",
    "\n",
    "    if SE+acc > best_SE+best_ACC:\n",
    "        best_SE, best_ACC = SE, acc\n",
    "        best_model.set_weights(model.get_weights())\n",
    "        pcnt = 0\n",
    "    else:\n",
    "        pcnt += 1\n",
    "    \n",
    "    print(\"Epoch: %d | SE: %.4f | Best SE: %.4f | ACC: %.4f | Best ACC: %.4f | AUC: %.4f | SP: %.4f\" %(e, SE, best_SE, acc, best_ACC, auc, SP))\n",
    "\n",
    "    if pcnt==patience:\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        acc = np.sum(y_pred==Y_test)/len(Y_test)\n",
    "        y_true = list(map(bin_label, Y_test))\n",
    "        y_pred = list(map(bin_label, y_pred))\n",
    "        auc = roc_auc_score(y_true, y_pred)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        SE = tp/(tp+fn)\n",
    "        SP = tn/(fp+tn)\n",
    "        print(mode+\" Test | SE: %.4f | ACC: %.4f | AUC: %.4f | SP: %.4f | valid SE: %.4f | valid ACC: %.4f\" %(SE, acc, auc, SP, best_SE, best_ACC))\n",
    "        with open(\"./result/\"+save, \"a\") as fw:\n",
    "            fw.write(\"SE: %.4f | ACC: %.4f | AUC: %.4f | SP: %.4f | valid SE: %.4f | valid ACC: %.4f\\n\" %(SE, acc, auc, SP, best_SE, best_ACC))\n",
    "        break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b9kELXUynUe5"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CVD_clasification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
